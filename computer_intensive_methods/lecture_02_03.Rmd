---
title: "Lecture 2 & 3"
output: "html_notebook"
---

# Approximation Probability and Distribution Functions
## Approximate Transformation of Random Variables

- Use the relationship between random variables to have an easier approximation.
- Eg. approximate chi-square random variable with standard normal distribution.

```{r}
# generate a random sample of size n of chi squares values
chisq_approximation <- function(n, p){
  (qnorm(p) + sqrt(2*n -1))^2
}

n <- 10
p <- c(0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95)
appr <- chisq_approximation(n = n, p = p)
exact <- qchisq(p, n)
diff <- exact - appr
x <- data.frame(p=p, 
                approximation=appr,
                exact=exact, 
                difference=diff
                )
x
```

Seems like quite close for this two.

## Closed Form Approximation
- [closed-form meant the formula can be evaluated in a finite number of standard operations](httpsen.wikipedia.orgwikiClosed-form_expression)
- used to fit an approximating function that is simpler in form but models the behavior of a given probability function F(X)
- polynomial or rational fraction approximating function is typically used.
- probabilities of F distribution may be computed using this type of approximation.
- [Horner's rule](httpsen.wikipedia.orgwikiHorner%27s_method)

```{r}
horner_computation <- function(x, a){
  
  n <- length(a)
  h <- a[n]*x + a[n-1]
  for (i in (n-2):1){
    h = h*x + a[i]
  }
  return(h)
}

a <- c(1, 0.278, 0.23, 0.001, 0.078)
x <- seq(0, 1, 0.2)
horner_computation(x = x, a = a)
```
## General Series Expansion
- Not going through in details
- First few terms is used in general, truncation.
- In tutorial got explanation about the Taylor series expansion
- Cornish-Fisher normalizing expansion

## Exact Realtionships between Distributions
- F and Beta are related through cdf.

## Numerical Root Finding
- given cdf F(x) and good algorithm evaluating F(Xx), to compute the percentage point $x_p$, for given p, the standard numerical root finding methods may be used
  - F(x) - p = 0
- General Methods (not for global root)
  - Newton's method
  - Secant method (have potential numeric problems)
  - Bisection method (converge slowest among 3 but does not use derivative and cdf  pdf)
  - Convergent criteria can be based on $x_{i+1} - x_i$ and  or $f(x_{i+1})$ or functions of these functions
  
```{r}
root_finding_func <- function(x){
  pnorm(x) - 0.95
}

bisection_root <- function(x){
  y = vector()
  y[1] = root_finding_func(x[1])
  y[2] = root_finding_func(x[2])
  
  if (sign(y[1]) != sign(y[2])) 
    {
    while (abs(root_finding_func(x[2]) - root_finding_func(x[1])) > 1.0e-6)
      {
      new <- sum(x) / 2
      if (root_finding_func(new) != 0.0)
        {
          y = root_finding_func(new)
          
          if (y*root_finding_func(x[1]) < 0) 
            x[2] <- new

          if (y*root_finding_func(x[2]) < 0)
            x[1] <- new
          }
      }
    } else
      {
        print('The interval selected does not contain the root. Please select another interval.')
      }
  answer <- new
  return(answer)
}
x <- c(1.5, 2)
bisection_root(x)
```
## Continued Fractions
- For pdf and cdf.
- This type of expansion for some functions are utilized by statistical computing algorithms.
- These fractions provide an accurate and efficient means for approximating the function which gives rise to the continued fraction.
- Methods to evaluate continued fraction
  - Forward Recurrence is used together with a convergence criterion to obtain adequate  approximation if number of iterations is not known.
  - Backward Recurrence is more resistant to rounding errors.
  
## Gaussian Quadrature
- class of methods for numerical integration
- Different Quadrature (depends on a and b range for integration)
  - Legendre-Gauss (finite range of a and b)
  - Jacobi-Gauss
  - Laguerre-Gauss
  - Hermite-Gauss
  - Chebyshev-Gauss
  - w(x) is used for the convergence and also determine which Quadrature to use.
  - lead to truncation of the interval or transformation of the interval
  - composite quadraturee may also be used where the interval of integration is divided into several subintervals with low-order quadrature formula for each subintervals.
- Statmod (gauss.quad)
- [slide 13](httpsmath.unm.edu~jehanzebfilesteachingfa19math375lecture17.pdf)
  - for general range of (-1, 1) it's just applying gauss.quad
  - when the range become (-3, 3) it's just doing a transformation where it become 3  integration of range (-1, 1) 3t with respect to t.
  - that why it become dnorm(3 a$nodes) 3
  - usually the approximation should be less than 1
```{r}
library(statmod)
gauss_approx <- vector()
for (n in 2:6)
  {
  a <- gauss.quad(n = n, kind='legendre')
  func <- dnorm(3* a$nodes)* 3
  gauss_approx[n] <- sum(a$weights * func)
  
}
gauss_approx

pnorm(3) - pnorm(-3)
```
## Newton-Cotes Quadrature
- Adjusted to equally spaced (for codes it's in terms of nodes)
- Do refer to the link in the slide 13 to understand more on gaussian quadrature.
- Can be used to approximate the P(a < x < b)

## Normal Distribution
- Most mathematical software provide evaluation of error function associated with the normal distribution and its complement
- $$erf(x) = 2(\pi)^{-1/2} \int_{0}^x exp(-t^2) dt$$
- $$erfc(x) = s(\pi)^{-1/2} \int_{x}^\infty exp(-t^2) dt = 1 - erf(x) $$
- P(-x < y < x), Y~N(0, 1/2)
- pnorm() $\Phi{(x)}$
- More accurate method - use a rational fraction approximation with approximately 7-decimal place accuracy
- gauinv()
- to solve $1 - \Phi{(x)} = p$

```{r}
## closed form approximation technique
c_vector <- c(2.515517,
              0.802853,
              0.010328
              )
d_vector <- c(1,
              1.432788,
              0.189269,
              0.001308
              )
p <- 0.05
t <- (-2*log(p))^(1/2)
numerator <- horner_computation(x=rep(t, length(c_vector)-1),
                                a=c_vector)[1]
denominator <- horner_computation(x=rep(t, length(d_vector)-1),
                                  a=d_vector)[1]
y_p <- t - numerator / denominator
if (p < 0.5)
  y_p = -y_p

y_p

qnorm(0.05)
```

```{r}
## need further look into it
error <- 1
x_p <- 0


gauinv <- function(p, x_p, error){
  
  lim <- 10e-20
  p_vector <- c(-0.322232431088,
                -1.0,
                -0.342242088547,
                -0.0204231210245,
                -0.453642210148*10e-4
                )
  q_vector <- c(0.0993484626060,
                0.588581570495,
                0.531103462366,
                0.103537752850,
                0.38560700634*10e-2
                )
  if (p > 0.5) 
    p = 1 - p
  
  if (p < lim)
    return(x_p)
  
  error <- 0
  
  if (p == 0.5)
    return(x_p)
  
  y <- log(1/p^2) ^ (1/2)
  y <- rep(y, length(p_vector)-1)
  numerator <- horner_computation(x = y,
                                 a = p_vector)[1]
  denominator <- horner_computation(x = y,
                                   a = q_vector)[1]
  x_p = y[1] + numerator / denominator
  
  if (p < 0.5)
    x_p = - x_p
  
  return(x_p)
}

gauinv(0.05, 0, 1)
qnorm(0.05)
```
## Students' T Distribution
- $$P(t|n) = 2[1 - T(t|n)]$$
- $$P(t|n) = 2f(t|n)(\frac{\sqrt{n}}{y}) [\frac{1}{n} + \frac{y^2}{2(n+2)} + \frac{1*3y^4}{2*4(n+4)} + \frac{5y^6}{16(n+6)} + ...] $$
- where f(t|n) is the pdf of t distribution
- we are using this to approximate T distribution T(t|n) so that we don't have to go through the integration step.

```{r}
t_approximation <- function(t, n){
  u = 1 + t^2/n
  p = c(0)
  q = c(0)
  q[2] = 2*dt(t, n)*(n*u)^(1/2)
  p[2] = q[2] / n
  
  error <- abs(p[2] - p[1])
  k <- 2
  i <- 2
  while (error > 1.0e-5)
    {
    q[i+1] <- q[i] * (k-1) / (k*u)
    p[i+1] <- p[i] + q[i+1] / (n+k)
    error <- abs(p[i+1] - p[i])
    k <- k + 2
    i <- i + 1
  }
  return(p)
}

n <- 15
t <- c(2)
t_approximation(t = t, n = n)
```

## Beta Distribution
- $$I_{x}(a, b) = \frac{1}{\beta(a, b)} \int_{0}^{x}{y^{a-1}(1-y)^{b-1} dy}$$ where 0<=x<=1, a>b, b >0
- beta distribution is also related to several other important distributions such as
  - Binomial $I_{p}(a, k-a + 1)$ where k is the n in binomial.
  - Negative binomial $I_{q}(a, k)$ where k is n-j + 1 and q is the probability of failure.
  - Student's t $I_{x}(\frac{n}{2}, \frac{1}{2}) = 2[1 - T(t|n)]$ where $x = \frac{n}{n+t^2}$.
  - F $I_{x}(\frac{n_2}{2},\frac{n_1}{2}) = 1 - F(y|n_1, n_2)$ where $x = \frac{n_2}{n_2 + n_1y}$
- $I_{x}(a, b)$ is pbeta(x, a, b)

## Chi Square Distribution
- chi-square and poisson
- $$1 - H(x|n) = \sum_{j}^{t-1}{\frac{m_j}{j!}e^{-m}}$$
- $t = \frac{n}{2}, m=\frac{x}{2},$ n is an even integer
- H(x|n) is given by pchisq(x, n)

```{r}
x <- 5
n <- 6
m <- x/2
j <- 0:(n/2-1)

chisq_cdf_approximation <- function(x, n){
  m <- x/2
  j <- 0:(n/2 - 1)
  return(1 - sum(m^j/factorial(j) * exp(-m)))
}

chisq_cdf_approximation(x, n)
pchisq(x, n)
```

