---
title: "Lecture 4, 5 & 6"
output: html_notebook
---

# Generating Random Numbers


```{r}
rand_congruential <- function(x0, alpha, c, m){
  random_numbers <- vector()
  x1 <- (alpha * x0 + c) %% m 
  
  while (!(x1 %in% random_numbers)){
    random_numbers <- c(random_numbers, x1)
    x1 <- (alpha * x1 + c) %% m
  }
  return(random_numbers)
}

x0 <- 0
alpha <- 2
c <- 3
m <- 10

rand_congruential(x0, alpha, c, m)
length(rand_congruential(x0, alpha, c, m))



x0 <- 2
alpha <- 5
c <- 1
m <- 2^4
 
rand_congruential(x0, alpha, c, m)
length(rand_congruential(x0, alpha, c, m))

x0 <- 7
m <- 10
rand_congruential(x0, x0, x0, m)
```
# Coupled Generators
## Method 1
X usually follow a binomial or poisson distribution
Y usually just use sample 1 to k
```{r}
set.seed(100)

k <- 128
n <- k
size <- k
prob <- 0.5
random_sample_size <- 100

x <- rbinom(n, size, prob)
t <- x[1:k]
R <- vector()

for (i in 1:random_sample_size){
  y <- sample(1:k, 1)
  R <- c(R, t[y])
  t[y] <- rbinom(1, size = size, prob = prob)
}
R
```
## Method 2
X usually follow uniform distribution because from 0 to 1 and we applied the floor function.
```{r}
set.seed(123)
k <- 128
n <- k
random_sample_size <- 100

x <- runif(n)
t <- x[1:k]
R <- vector()

for (i in 1:random_sample_size){
  j <- floor(k*(sample(x, 1))) + 1
  R <- c(R, t[j])
  y <- runif(1)
  t[j] <- y
}
R
```
# Criteria for a Good Generator
- sequence of random numbers generated must follow the uniform (0, 1) distribution (true only after scale the random generator number slide 4)
- sequence of random numbers generated must be statistically independent
- sequence of  random numbers must be reproducible to allow for replication of the simulation experiment
- sequence must be non-repeating for any desired length. Although not theoretically possible, along repeatability cycle is adequate for practical purposes.
- generation of the random numbers must be fast - time and cost
- technique used in generating random number should require little computer memory.

# Test of Random Number Generators
- Theoretical tests based on mathematical structure of the generator
- Empirical Test examine a sample of numbers generated by generator

# Methods
Focus on test are here
- Chi-square Test
- Gap Test
- Run Test
- Serial Correlation Test

## Chi-Square Test
- To check whether the generated sequence may be regarded as coming from a uniform (0, 1) distribution.
- chop the interval into equal probability instead of equal length
- so we will be chopping on the cdf
- a small value of chi-square implies a good fit
```{r}
# R is random sequence
p <- ecdf(R)
k <- 10
cdf <- sort(p(R))
R <- sort(R)
n <- length(R)
# cut the interval into equal probability
interval <- cut(cdf, seq(0, 1, by = 1/k), include.lowest = T)
input <- as.data.frame(table(interval))
# since we already have the table, they should expect how much using chi-square easily
chisq.test(table(interval))

# raw method
exp <- rep(n*(1/k), length(input))
chi_sq <- sum((exp - input[, 2])^2 / exp)
p_value <- pchisq(chi_sq, df=length(input[,2])-1, lower.tail=F)
p_value
```
## Kolmogorov-Smirnov Test
- empirical cdf (i.e, your sample cdf) = i/n
- theoretical cdf = x
- large value of D is poor fit
- low p-value indiciates the distribution are the same
```{r}
set.seed(888)
n <- length(R)
theory.x <- sort(runif(n))
theory.p <- ecdf(theory.x)

empirical.x <- R
empirical.p <- ecdf(empirical.x)

diff <- empirical.p(empirical.x) - theory.p(theory.x)
largest_diff <- max(abs(diff))
largest_diff

ks.test(theory.x, empirical.x)
```
### Guided Exercise
```{r}
random_sample <- c(0.72, 0.57, 0.60, 0.91, 0.44, 0.32, 0.53, 0.29, 0.98, 0.51, 0.94,
0.83, 0.65, 0.81, 0.76, 0.11, 0.63, 0.96, 0.87, 0.79)
sorted_random_sample <- sort(random_sample)
sorted_random_sample

# self test
empirical.cdf <- seq(1, length(random_sample))/length(random_sample)
d <- max(abs(empirical.cdf - sorted_random_sample))
d

# build in
ks.test(random_sample, "punif")
```
http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/
if the d > critical value, then we reject null hypothesis, 
if we do not reject null hypothesis, then the distribution follow the uniform distribution. (we prefer this)

KS-test is easier compared to Chi-Square Test
- procedure in ks-test is easier
- ambiguity due to different sub-interval in chi-square

## Gap Test
- test the randomness of numbers in a sequence
- look in the notes for explanation.
- our focus is on the gap itself, whether the gap distribution, R should follow geometric distribution.

### Guided exercise
```{r}
set.seed(888)
alpha <- 0.20
beta <- 0.29
n <- 20
gap <- 0
gap_vector <- vector()
y <- c(runif(1))
first_indicator <- 0


# to trace the first number to be in the criteria
# that why it's a Un not U1
while (first_indicator==0){
  first_index <- length(y)
  if ((y[first_index] > alpha) & (y[first_index] < beta)){
    first_indicator <- 1
    counter <- 0
  } else {
      y <- c(y, runif(1))
    }
print(first_index)
}

# after getting first number
# generate random numbers
# add to gap when it's in the range
while (gap <= n){
  y <- c(y, runif(1))
  index <- length(y)
  gap_vector <- c(gap_vector, gap)
  
  if ((y[index] >= alpha) & (y[index] <= beta)){
    gap <- gap + 1
  }
  
}

# because gap 0 is actually our first gap
# then the last number generated is gap 21
gap_vector <- gap_vector + 1
gap_vector <- gap_vector[gap_vector <= n]
```
```{r}
# to remove the first count every time
gap_length <- table(gap_vector) - 1
input <- as.data.frame(table(gap_length))
input
input$gap_length <- as.numeric(input$gap_length)

breaks <- c(0,3,14,max(input$gap_length))
input$tags <- cut(input[,1], breaks = breaks, include.lowest = T)
input <- as.data.frame(table(input$tags))
p <-  (1 - (beta - alpha)) ^ seq(1,length(input[,2]))
chi_sq <- sum((input[, 2]-n*p)^2 / n*p)
chi_sq

pchisq(q=0.01, df=length(input[, 2]),lower.tail = F)

result <- chisq.test(input[,2])
result
result$expected

```

```{r}
library(randtoolbox)
gap.test(y, lower=alpha, upper=beta, echo=T)
```

## Run Test
- examine the arrangement of numbers in a sequence to test whether the numbers are uniformly and independently distributed.
- too many or too less run = not likely to random

```{r}
# r be total number of runs in a random sequence
R <- 0

# sample size
N <- 0
mean <- (2*N - 1) / 3
sigma_square <- (16*N - 29) / 90

z <- (r-mean) / sqrt(sigma_square)

```
### Guided exercise
```{r}
x <- c(0.42, 0.92, 0.80, 0.68, 0.59, 0.44, 0.62, 0.43, 0.78, 0.79, 0.88, 0.90, 0.95)
n <- length(x)
y <- as.numeric(x[-1] > x[-n])
run_counter <- vector()
counter <- 1
for (i in 2:length(y)){
  if (y[i] != y[i-1]){
    run_counter <- c(run_counter, counter)
    counter <- 1
  } else {
    counter <- counter + 1
  }
  
}
run_counter <- c(run_counter, counter)
run_counter

R <- length(run_counter)
N <- length(x)
mean <- (2*N - 1) / 3
sigma_square <- (16*N - 29) / 90
z <- (R-mean) / sqrt(sigma_square)
z
```

# Non-Uniform Random Numbers
- use cdf to generate non-uniform random numbers
- can only use when cdf can be inverted
