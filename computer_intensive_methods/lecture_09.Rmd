---
title: "R Notebook"
output: html_notebook
---

Guided Exercise
```{r}
x <- c(2, 10, 4, 7)
m <- c(x[1])
s <- c(0)

for (j in 1:(length(x)- 1)){
  m[j+1] <- m[j] + (x[j+1] - m[j]) / (j+1)
  s[j+1] <- (1 - 1/j)*s[j] + (j+1)*(m[j+1] - m[j])^2
  
}
m
s
```

Exercise
mean = 3, meant for the paramter is = 1/3
```{r}
set.seed(128)

d <- 0.05
n <- 30
y <- rexp(n, rate=1/3)
sd(y)

while (sd(y)/sqrt(n) > d){
  y1 <- rexp(1, rate=1/3)
  y <- c(y, y1)
  n <- length(y)
}
mean(y)
length(y)
```
```{r}
set.seed(128)
d <- 0.05
n <- 30
y <- rexp(n, rate=1/3)

m <- c(mean(y))
s <- c(sd(y))

while ((s[length(s)] / sqrt(n)) > d){
  y1 <- rexp(1, rate=1/3)
  y <- c(y, y1)

  
}
mean(y)
length(y)
```
Monte Carlo Simulation
```{r}
set.seed(123)
theta <- 365
n <- 20
k <- 1000
theta1 <- c(0)
theta2 <- c(0)

for (j in 1:k){
  x <- rexp(n, 1/theta)
  theta1[j] <- 2*mean(x) - 1
  theta2[j] <- ((n+1)*max(x)/n) - 1
}

mse1 <- sum((theta1 - theta)^2) / k
mse2 <- sum((theta2 - theta)^2) / k
eff <- mse2 / mse1
mse1
mse2
eff
```
The one with smaller mse will be the better estimator. Hence MSE for theta 1 is a far lower. Hence theta 1 is better estimator. We can use eff to evaluate the estimator.

```{r}
k <- 1
j <- 1
u <- runif(1)


```

